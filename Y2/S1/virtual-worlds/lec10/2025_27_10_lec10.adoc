= Virtual Worlds - Reinforcement Learning
:toc:
:toc-title: Contents
:nofooter:
:stem: latexmath

== What is reinforcement learning?

ML paradigm where an agent (AI model) learns by interacting with an environment

* correct actions are rewarded
** rewards may be delayed
* incorrect actions are penalized

Agents learn by trial and error

* eventually they should converge to (one of) the correct sequence(s) of actions

.Pseudocode
[source,python]
----
initial_state = agent.observe(environment)

for t in timesteps:
    state = agent.observe(environment)
    action = agent.select(actions, t)
    new_state, reward = environment.update()
    agent = agent.reward(reward)
    state = new_state
----

== Why reinforcement learning?

. Can be used for training
* no real patients
* no harm done
. Infinite data
* simply generate more scenarios
. High level of control
* isolate specific challenges/scenarios
. Rare event simulation
* can simulate very uncommon events that would otherwise be difficult to train for/on
. Cheap(ish)
* no hardware required
* software only
** unless you need to test with physical agents in which case good luck

== RL components

4 main components:

. State stem:[s]
* environment description
** position
** patient vitals

. Action stem:[a]
* available choices
** discrete (move 1 unit left/right/forward/backward)
** continuous (apply 1 n/m of force to this location)

. Reward stem:[r]
* feedback signal
** higher = more rewarding
** lower = less rewarding
* need to decide reward frequency
** more frequent = easier to learn
*** can be a trap

. Policy stem:[\pi]
* behavior function
** maps states to actions
* can be deterministic (state stem:[S] will always lead to action stem:[A])
** stem:[\pi: S \rightarrow A]
** stem:[a = \pi(s)]
* can also be stochastic (state stem:[S] has a probability to allow for action stem:[A])
** each action has a probability
** stem:[\pi: S \cdot A \rightarrow [0, 1\] ]
** stem:[\pi(a|s) = P(A_t = a | S_t = s)]

== Markov Decision Processes

Framework for modeling decision making

Defined by the tuple:

[stem]
++++
\text{MDP} = (\mathbf{S}, \mathbf{A}, P, R, \mathbf{\gamma}) 
++++

Where:

* stem:[\mathbf{S}] is the set of states
* stem:[\mathbf{A}] is the set of actions
* stem:[P] is the state transition probability function
** see definition below
* stem:[R] is the reward function
** the function that returns the reward
** defined below
* stem:[\gamma] is the discount factor
** value in stem:[0 \leq \gamma \leq 1]
** reduces the value of future rewards
** useful for ongoing tasks (i.e. tasks without a definite end)
*** ensures that the final sum stem:[G] will always be a finite sum
*** creates a preference for short term rewards (not necessarily good but can help in some situations)

.State transition probability function
[stem]
++++
P(s' | s, a) = Pr \{    S_{t+1} = s' | S_t = s, A_t = a      \}
++++

Where:

* stem:[S_t] is a random variable that can take any value in the state space (the state at time stem:[t])
** stem:[s] is the value given to stem:[S] state at time stem:[t]
* stem:[A_t] is a random variable that can take any value in the action space (the action taken at time stem:[t])
** stem:[a] is the actual value of the action stem:[A] at time stem:[t]
* stem:[s'] is a different state
** not necessarily new
** it could be to stay in current state

Basically:

* if the system is in state s right now and I take action stem:[a], the chance that it will end up in state stem:[s'] on the next time step is stem:[P(s'|s,a)]

.Reward function
[stem]
++++
r = R(s, a, s')
++++

* what is the expected reward after transitioning from one state to the next?

=== Value functions

There are 2 main value functions:

.1. State-value function
[stem]
++++
V^{\pi} (s) =
    \mathbb{E}_{\pi} [G_t | S_t = s] \\

    \text{where:} \\
 
    \mathbb{E}_{\pi} [G_t | S_t = s] = \mathbb{E}_{\pi} \bigg[\sum_{k=0}^{\inf} \gamma^{k} R_{t+k+1} | S_t = s    \bigg]
++++
* describes the expected return starting from state stem:[s] while following policy stem:[\pi]
** the value is equal to the expected value of the final sum given the state
** where the final sum is the discounted sum of all the time steps

.2. Action-value function
[stem]
++++
Q^{\pi} (s, a) = 
    
    \mathbb{E}_{\pi} [G_t | S_t = s, A_t = a] \\

    \text{where:} \\
 
    \mathbb{E}_{\pi} [G_t | S_t = s, A_t = a] = \mathbb{E}_{\pi} \bigg[\sum_{k=0}^{\inf} \gamma^{k} R_{t+k+1} | S_t = s, A_t = a    \bigg]
++++
* describes the expected return starting from state stem:[s] and taking action stem:[a] while following policy stem:[\pi]
** the value is equal to the expected value of the final sum given the state and the action taken
** where the final sum is the discounted sum of all the time steps and the action taken

These can be related to each other (since they're basically the same except for the action component)

.Relation
[stem]
++++
V^{\pi}(s) = \sum_{a \in A} \pi (a | s) Q^{t}(s, a)
++++

* the value sum is equal to the sum of the policy of the action given the state times the action value for all possible actions in the action space

There is an optimal policy stem:[\pi^*], that is given by the following formulation:

[stem]
++++
\pi^* (s) = \text{arg} \max_{a'} Q^*(s, a)
++++

Which makes it easy since to get the best possible policy we can just select the action with the highest stem:[Q]

* makes stem:[P] and stem:[R] irrelevant

=== Exploration vs exploitation

Should a model *exploit*

* always choose the best known action (farm easy rewards)

Or should it *explore*

* try different actions to potentially find a better strategy

Typically models are set to be highly explorative at first but become more exploitatitve over time

* find a good strategy quickly
* accumulate relatively early
* avoid constant exploration

== Q learning

Agents/models learn the optimal policy while following the exploratory policy

* derives policy from Q function

Governed by this update rule:

[stem]
++++
Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]
++++

Where:

* stem:[\alpha] is the learning rate (between 0 and 1, inclusive)
* stem:[r] is the observed reward
* stem:[\gamma] is the discount factor
* stem:[s'] is the next state

with Temporal Difference as the loss function

[stem]
++++
\delta = r + \gamma \max_{a'} Q(s', a') - Q(s, a)
++++

You could represent Q learning as a table/matrix of state/action but it would become intractable

* big
** chess has stem:[10^{43}] states
** continuous state spaces have infinite states
* memory not big
* not work
* unknown states also exist

Better to use the *convergence theorem*

Q-learning will always find a stem:[Q^*] with probability 1 given:

. all (state, action) pairs are visited infinitely often
. the sum of the learning rate at every time step is infinite AND the sum of the squared learning rate at every time step is not infinite

How do we find these functions?

* Neural networks (of course)

== Deep Q Networks

Approximate the stem:[Q] function using a neural network

[stem]
++++
Q(s, a; \theta) \approx Q^{*}(s, a)
++++

where stem:[\theta] are neural network parameters

=== DQN limitations

DQNs aren't magic

. Inefficient
* they need millions of environment interactions to be good
** humans might need a few dozen idk
. Discrete actions only
* continuous actions need to be discretized
. Overestimation bias
* max operator is optimistic
** initial stem:[Q] values are extreme
. Stability
* even with the appropriate modifications (below) DQNs can still diverge

They work best in discrete and high dimensional spaces and in (near) deterministic environments

* where stem:[\pi: A \rightarrow S] 
** a given action will (almost) always lead to a particular state

=== DQN architecture

. State representation
* e.g. game pixels
** this pixel is red
** this one is green
. CNN1
* e.g. (32 filters, 8x8, stride 4)
. CNN2
* e.g. (64 filters, 4x4, stride 2)
. CNN2
* e.g. (64 filters, 3x3, stride 1)
. MLP
* 512 units
. Output
* all actions with corresponding stem:[Q] values

=== DQN updates

The naive method is a typical loss function and gradient update combo

.Loss function
[stem]
++++
L(\theta) = \mathbb{E}[(r + \gamma \max_{a'}Q (s', a'; \theta) - Q(s, a;\theta))^2]
++++

.Gradient update
[stem]
++++
\theta \leftarrow \theta \alpha \nabla_{\theta} L(\theta)
++++

this has 3 main problems:

. Learning is unstable
* optimizers expect a fixed target parameters stem:[\theta]
** if stem:[\theta] changes the optimizer can overshoot or become unstable
* the gradient optimization problem becomes non convex
** gradient can explode
. Sequential states are highly correlated
* sequential states are often very similar
** moving left at t1 -> probably still moving left at t2
* can confuse the optimizer
** gradient changes in the same direction for 10 epochs
** optimizer doesn't know how to react -> unstable/slow learning
. Divergence
* single big rewards can explode the TD error
* the stem:[\max] operator can pick overly optimistic stem:[Q] values
** can lead network on a wild goose chase

2 methods to resolve/mitigate these issues:

.1. Experience Replay

Stores state transitions in a buffer

* stem:[(s, a, r, s')]

Takes random mini batches during training

* breaks consecutive sample correlation
* improves efficiency

.2. Target Network

Uses a different network with parameters stem:[\theta^{-}] that are used to compute target values

* the target becomes fixed
* wild goose chase avoided

the targets are calculated like this:

[stem]
++++
y = r + \gamma \max_{a'} Q(s', a'; \theta^{-})
++++

The target network weights are updated:

* every stem:[C] steps instead of at every step (hard update)
** stem:[\theta^{-} \leftarrow \theta]
* using stem:[\tau] (soft update)
** stem:[\theta^{-} \leftarrow \tau \theta + (1 - \tau) \theta^{-}]

Learning becomes stable

With these changes we can modify the loss and gradient update functions:

.MSTDE
[stem]
++++
L(\theta) = \mathbb{E}_{(s, a, r, s')} \sim D [(y - Q(s, a; \theta))^2]
++++

with stem:[y] equal to the target network target calculation

.Gradient update
[stem]
++++
\nabla_\theta L(\theta) = \mathbb[-2 (y - Q(s, a; \theta)) \nabla_{\theta}Q(s, a; \theta)] 
++++

== Policy gradient methods

Parametrizes the policy directly instead of the action(s) or state(s)

* works with continuous actions
* handles non deterministic policies

However:

* slower convergence
* higher variance
** less stable performance

=== Parametrized policies

A policy is represented as:

[stem]
++++
\pi_\theta (a | s) = P(a | s;\theta)
++++

* i.e. policy stem:[\pi_{\theta}] with action stem:[a] given state stem:[s] is equal to the probability of action stem:[a] given state stem:[s] parametrized by stem:[\theta]

This changes depending on whether we are using discrete or continuous actions

.Discrete actions
[stem]
++++
\pi_\theta (a | s) = \frac{e^{h(s, a; \theta)}}{\sum_{a'} e^{h(s, a'; \theta)}}
++++

This feeds the output logits into a softmax to decide which action to take

.Continuous actions
[stem]
++++
\pi_\theta (a | s) = \mathcal{N} (a; \mu_{\theta}(s), \sigma^2_\theta(s))
++++

This draws an action stem:[a] from a Gaussian distribution given the current state stem:[s]

* the mean and spread are functions of our state stem:[s]

== Actor-Critic Methods

You can also learn both:

* policy: stem:[\pi \theta (a|s)]
** actor
** selects actions
* value function: stem:[V_w(s)] or stem:[Q_w (s, a)]
** critic
** evaluates actions 

Requires estimating the *advantage*

* measure of how much better an action is compared to the average action in a given state
** state stem:[s_x] has stem:[y] available actions
** take the mean reward stem:[\sigma = \frac{\sum_i y_i}{y}]
** `advantage = {action: mean - action for action in state}`

Or:

[stem]
++++
\hat{A}_t = r_t + \gamma V_w(s_{t+1}) - V_w(s_t)
++++

i.e. advantage is equal to the reward at each time step plus the difference between the discounted value of the next step minus the current value

stem:[\hat{A}] is use in updating the actor and the critic

.Policy/actor update
[stem]
++++
\theta \leftarrow \theta + \alpha_1 \hat{A}_t \nabla \theta \log \pi \theta(a_t | s_t)
++++

.Value/Critic
[stem]
++++
w \leftarrow w + \alpha_2 \delta_t \nabla_w V_w(s_t)
++++

where stem:[\delta_t] is the TD error

[stem]
++++
\delta_t = r_t + \gamma V_w(s_{t+1}) - V_w (s_t)
++++
