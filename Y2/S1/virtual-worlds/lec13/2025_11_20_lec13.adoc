= Virtual Worlds - Human Pose Estimation
:toc:
:toc-title: Contents
:nofooter:
:stem: latexmath

== Taxonomy

2D HPE -> estimate x, y coordinates of each keypoint

3D HPE -> estimate x, y, z coordinates of each keypoint

Single person/multi person

Most modern methods are multi person

Early models used regression on images

* DeepPose (2014) worked but it wasn't especially accurate
** lots of pooling
** lots of operations that disrupted feature locality

== Heatmap based 2D HPE

Instead of predicting the exact x, y coordinate (very hard)

* need continuous x, y balanced dataset

Given a keypoint -> decide, for each pixel, if that pixel belongs to that keypoint

* returns probability values
* add gaussian filter to convolution
** generates a heatmap over the point -> high prob near middle -> low prob near edges

Can use pure convolution

* no pooling required -> no loss of locality

High quality feature maps required

== HPE Metrics

=== PCK/PDJ

Percentage of correct keypoints

Keypoint i is correctly detected if the distance between the predicted pos and real pos < threshold

* distance can be in
** pixels (arbitrary, need to pick a threshold relative to the scale of the real pose)
** absolute units

=== OKS

Object Keypoint Similarity

Like IoU

Overlap between the ground truth box and the predicted box

* quantifies closeness between GT and P
* in range stem:[(0, 1)]
** enables computation of average precision (AP) and average recall (AR)

For keypoint stem:[i]

* exponent of the difference between true and predicted divided by the object scale (squared) per some constant stem:[k]
** dataset curator(s) suggest values of stem:[k] so it doesn't come from training
*** stem:[k] is higher for parts that are more difficult to segment

.OKS formulation
[stem]
++++
\text{OKS} =
\frac
{
    \sum_i \exp \biggl(-\frac
    {d^2_i}{2s^2 \cdot k_i^2}\biggr)
    \mathbb{I} [v_i \gt 0]
}    
{
    \sum_i \mathbb{I} [v_i \gt 0]
}
++++

where:

* stem:[d_i] is the Euclidian distance between the predicted point and the ground truth point
* stem:[s] is the scale of the object
** i.e. stem:[\sqrt{\text{area of segmentation}}]
* stem:[k_i] is a constant that controls falloff
* stem:[v_i] is the ground truth visibility flag
** COCO:
*** stem:[v_i = 0 \implies] not visible
*** stem:[v_i = 1 \implies] labeled but not visible
*** stem:[v_i = 2 \implies] visible

i.e.:

* while ignoring all invisible and unlabeled keypoints (stem:[\mathbb{I} [ v_i \gt 0 \]])
* pass the distance stem:[d_i] through a Gaussian to give a measure of similarity
** Gaussian standard dev is stem:[\sigma = sk_i]
** similarity is value in stem:[[0, 1\]], where stem:[d_i \sim 0] is very good similarity

== Multi person 2D HPE

Predict keypoint positions for each person in an image

* identify which keypoint belongs to which person

Challenges:

* unknown number of persons
* scale/orientation
* occlusion
** partial visibility
** crowds

Methodologies:

* top down: find persons, then keypoints per person
** slower but more accurate
* bottom up: find keypoints then keypoint owners
** faster but less accurate
* one stage: gets both keypoints and people in one shot
** based on bottom up

=== Papandreou et al. (2017)

Defined a 2 stage top down approach:

. Person detection (standard object detection)
.. Cropping around person bounding box
. Mixed classification/regression
* Fixed aspect ratio crop
* using a fully convolutional model (ResNet-101)
* Outputs stem:[3k] output maps
** stem:[k] is the number of joints
** 1 class heatmap -> disk indicating possible joint location (classifies whether the joint is inside or outside the disk)
** 2 offset heatmaps -> x and y offsets that define how far you need to move to reach the predicted joint location
*** vector field that points pixels towards the joint
** pixels within the offset maps vote on where the joint really is
*** weighted average of the candidate locations

.Weighted average of estimated positions
[stem]
++++
F_k (x_i) = \sum_j \frac{G(x_j + F_k(x_j) - x_i) h_k(x_j)}{\pi R^2} 
++++

where:

* stem:[x_i] is the position in which to compute the output value
* stem:[x_j] are other positions
** typically near stem:[x_i]
* stem:[x_j + F_k(x_j)] is the estimated position of the stem:[k_{th} joint]
** stem:[x_j + ] offset
* stem:[x_j + F_k(x_j) - x_i] is the distance between the estimate and stem:[x_i]
* stem:[G] is a linear interpolation kernel
** 1 pixel wide tent function around stem:[x_i]
** stem:[G(0, 0) = 1]
* stem:[G(x_j + F_k(x_j) - x_i)] is the the contribution to the vote
** how confident it is that stem:[x_i] is the joint location
* stem:[h_k(x_j)] is the condidence of the predictions
** weighs the vote
** high confidence = heavier

They also used 2 new evaluation metrics:

. Keypoint-based confidence score/Pose Rescoring
* combines the confidence scores of each keypoint
* stem:[\text{score}(I) = \frac{1}{K} \sum_{k=1}^{K} \max_{x_i} F_k (x_i)]
. Keypoint-based non-maximum suppression (OKS-NMS)
* Measure overlap using OKS instead of IoU
* detects true/false positives better

=== OpenPose (2017)

Bottom up approach

* untangles complexity from number of people in image
** constant time performance regardless of number of people (!!!)
* better real time performance

Based on *Part Affinity Fields*

* 2D vector fields over image domain
* encodes location and orientation of limbs
** basically arrows that point to where the limb is oriented
** requires finding limb first
* Position and orientation makes them better than simple point estimates

Uses FFCNN

. predicts *part confidence maps*
* measure of the belief that a part is located at a given pixel
** "I'm 90% sure that pixel (5325, 1441) is part of the elbow"
* stem:[S = (s_1, s_2, \dots, s_J)  \subset [  0, 1  \]^{HW} ]
** stem:[s_J] is a 2D array representing the confidence map of part stem:[j]
*** each element is between 0 and 1 (0 = not confident at all, 1 = perfectly confident)
** stem:[[  0, 1  \]^{HW}] is a set of vectors of length stem:[HW] where each vector can have values between 0 and 1
*** basically the set of all stem:[s_J] arrays

. also predicts the PAFs
* encodes the degree of association between part pairs
** "These 2 parts are very linked", "these 2 parts may or may not be linked"
* stem:[L = (L_1, L_2, \dots, L_C) \subset \mathbb{R}^{2HW}]
** stem:[C] is the number of vector fields where the length is equal to the number of connected part pairs
*** like the mediapipe skeleton?
** Each pixel in stem:[L_i] is a 2D vector
*** encodes part orientation

These ensure that overlapping limb keypoints are appropriately mapped to their rightful owners

* 2 overlapping points with parts having different directions are likely to belong to different people

==== Multiple Bipartite Matching

Generating poses for multiple people is called *multiple bipartite matching*

Uses *graphs* (because of course it does)

* nodes are detections
* edge weights are scores that quantify how much the detections fit
** line integration over PAF

.Multiple Bipartite Matching
[stem]
++++
E = \int_0^1 \mathbf{L}_c( \mathbf{p}(u) ) \cdot \frac{ \mathbf{d}_{j2} - \mathbf{d}_{j1} }{ || \mathbf{d}_{j2} - \mathbf{d}_{j1} ||_2 } du
++++

where:

* stem:[\mathbf{p} (u) = (1 - u) \mathbf{d}_{j1} + u \mathbf{d}_{j2}]
** is the interpolation of the position of the 2 parts stem:[\mathbf{d}_{j1}] and stem:[\mathbf{d}_{j2}]
** which can be discretized as a sum over N uniformly sampled points between stem:[\mathbf{d}_{j1}] and stem:[\mathbf{d}_{j2}]

WARNING: connecting any joint to any other joint is an NP hard problem. Greedily connect nearby joints only to relax the problem

=== Associative Embeddings (2017)

One-stage method

* detects and groups keypoints at the same time(ish)

The network predicts an *identity tag* and a per pixel detection score (classic heatmap)

* identity of group assignment
** this bit belongs to this group
*** a group can be a person but can also be other things (animals, plants, arbitrary objects)

Given an image:

[source,python]
----
def predict(image):
    predictions = []
    for keypoint in image:
        heatmap = detection_hm(keypoint)
        embedding = associative_embedding(keypoint)
        predictions.append((heatmap, embedding))
----

Uses MSE loss for detection heatmaps

Uses *Grouping Loss* for tags

* made up of multiple parts

==== Grouping Loss

.1. Reference embedding
[stem]
++++
\overline{h} = \frac{1}{K} \sum_{k} h_k(x_{nk})
++++

* gets reference embedding for person stem:[n]
* mean of predicted embeddings for all joints of a person

.2. Intra-person term
[stem]
++++
\frac{1}{NK} \sum_n \sum_k (\overline{h}_n - h_k (x_nk))^2
++++

* minimizes variance between joint embeddings of the same person
** pulls them close to the reference embedding

.3. Inter-person term
[stem]
++++
\frac{1}{N^2} \sum_n \sum_{n'} \exp \{  - \frac{1}{2 \sigma ^2} (\overline{h}_n - \overline{h}_{n'})^2 \}
++++

* pushes reference embeddings belonging to other people away
** exponentially decaying penalty
** closer => higher penalty (exponential decrease with distance)

where:

* stem:[x] is the pixel location
* stem:[h_k \in \mathbb{R}^{HW}] is the predicted tag heatmap for the stem:[k]th joint
* stem:[\{ (x_{nk}) \}] with stem:[n=1, \dots, N] and stem:[k=1, \dots, K] are the ground truth values
** stem:[x_{nk}] is the ground truth pixel location of joint stem:[k] of person stem:[n]

== Single person 3D HPE

Given an RGB image you can get 2 things:

* skeleton based -> 3D keypoints
* model based -> 3D mesh/model
** can infer keypoints from surface

Use volumetric heatmaps instead of 2D heatmaps

Uses *Geodesic Point Similarity* instead of OKS

* similar to OKS
* works on 3D data

.GPS

[stem]
++++
\text{GPS}_j = \frac{1}{|P_j|} \sum_{p \in P_j} \exp \biggl(  \frac{-g(i_p, \hat{i}_p)^2}{2k^2}  \biggr)
++++

Where:

* stem:[P_j] is the annotated ground truth set for person stem:[j]
* stem:[i_p] is the estimated vertex position at point stem:[p]
* stem:[\hat{i}_p] is the ground truth vertex position at point stem:[p]
* stem:[g()] is the geodesic distance
** the shortest distance between 2 points on a surface
* stem:[k] is a normalization factor


The stem:[GPS] for person stem:[j] is equal to:

. the sum of:
. the exponentiation of:
.. the squared negative geodesic distance between the real point and predicted point
... normalized by the normalization factor stem:[k]
. multiplied by the number of points belonging to that person

=== Supervised 2D to 3D mapping

Getting 3D datasets in the wild is very hard

* get data from motion caption datasets
** videos+marker-based 3D poses
* project 3D pose to 2D after transforming to camera coordinates (inverse camera transform)
** predictable system
* normalize inputs and targets
** normalization location likely varies with implementation/model

Predict 3D pose from 2D pose

* full image not required
* inputs: 2D pose stem:[\mathbf{x} \in \mathbb{R}^{2K}]
* outputs: 3D pose stem:[\mathbf{y} \in \mathbb{R}^{3K}]
** stem:[K] is the number of keypoints
* can regress from 2D to 3D using FCN: stem:[\mathbf{y} = f(\mathbf{x, \theta})]

=== VNect (2017)

Skeleton-based single-person 3D HPE system

* creates a skeleton instead of a dense model
* heatmap-based approach
* uses pelvis as root

Overview:

. Full frame input
* big image
. Bounding box around target
* crop them out of FFI
. CNN regression
* confidence heatmap stem:[H_i]
** estimates 2D location of joint in image
* location maps
** estimates 3D joint positions relative to pelvis -> stem:[[ \mathbf{X_i, Y_i, Z_i} \]]
** find max pixel in stem:[H_i] -> use to find best in stem:[[ \mathbf{X_i, Y_i, Z_i} \]]
*** `p = np.argmax(h_i); maxs = [plane[p] for plane in x, y, z]`
. Temporal filter
* generates 2D keypoints
* can estimate 3D pose
. Skeleton fitting
* build skeleton

=== DensePose (2018)

Solves *dense surface correspondence*

* assigning each pixel to a position of a template body 
* much harder than predicting joint positions

Uses https://is.mpg.de/code/smpl[SMPL]

* learned 3D representation of human body surface
* very realistic
* easy to manipulate
** body shape
** body pose
** body location in space
* differentiable

Overview: 

. partition 3D surface into semantic parts
* parts with meaning
** torso, head, left leg
. classify which part each pixel belongs to
. regress pixel's 2D coords onto that part's UV map

Network architecture:

* Mask-RCNN + FPN
* finds ROI for people
* uses ROI-align pooling to extract features per region

The head of the network does 2 things:

* generates per pixel surface part classification
** this pixel belongs to part stem:[p_0]
** this pixel belongs to part stem:[p_1]

Very poor real time performance

* < 30 FPS using GTX 1080 at low resolutions
* might be better with modern GPUs but who knows

Ground truth data is sparse

* we'd rather have dense data

Use FCNN to paint areas near unannotated pixels

* using UV values