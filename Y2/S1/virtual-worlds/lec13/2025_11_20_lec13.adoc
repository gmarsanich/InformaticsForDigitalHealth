= Virtual Worlds - Human Pose Estimation
:toc:
:toc-title: Contents
:nofooter:
:stem: latexmath

== Taxonomy

2D HPE -> estimate x, y coordinates of each keypoint

3D HPE -> estimate x, y, z coordinates of each keypoint

Single person/multi person

Most modern methods are multi person

Early models used regression on images

* DeepPose (2014) worked but it wasn't especially accurate
** lots of pooling
** lots of operations that disrupted feature locality

== Heatmap based 2D HPE

Instead of predicting the exact x, y coordinate (very hard)

* need continuous x, y balanced dataset

Given a keypoint -> decide, for each pixel, if that pixel belongs to that keypoint

* returns probability values
* add gaussian filter to convolution
** generates a heatmap over the point -> high prob near middle -> low prob near edges

Can use pure convolution

* no pooling required -> no loss of locality

High quality feature maps required

== HPE Metrics

=== PCK/PDJ

Percentage of correct keypoints

Keypoint i is correctly detected if the distance between the predicted pos and real pos < threshold

* distance can be in
** pixels (arbitrary, need to pick a threshold relative to the scale of the real pose)
** absolute units

=== OKS

Object Keypoint Similarity

Like IoU

Overlap between the ground truth box and the predicted box

* quantifies closeness between GT and P
* in range stem:[(0, 1)]
** enables computation of average precision (AP) and average recall (AR)

For keypoint stem:[i]

* exponent of the difference between true and predicted divided by the object scale (squared) per some constant stem:[k]
** dataset curator(s) suggest values of stem:[k] so it doesn't come from training
*** stem:[k] is higher for parts that are more difficult to segment

== Multi person 2D HPE

Predict keypoint positions for each person in an image

* identify which keypoint belongs to which person

Challenges:

* unknown number of persons
* scale/orientation
* occlusion
** partial visibility
** crowds

Methodologies:

* top down: find persons, then keypoints per person
** slower but more accurate
* bottom up: find keypoints then keypoint owners
** faster but less accurate
* one stage: gets both keypoints and people in one shot
** based on bottom up

=== Papandreou et al. (2017)

Defined a 2 stage top down approach:

. Person detection (standard object detection)
.. Cropping around person bounding box
. Mixed classification/regression
* Fixed aspect ratio crop
* using a fully convolutional model (ResNet-101)
* Outputs stem:[3k] output maps
** stem:[k] is the number of joints
** 1 class heatmap -> disk indicating possible joint location (classifies whether the joint is inside or outside the disk)
** 2 offset heatmaps -> x and y offsets that define how far you need to move to reach the predicted joint location
*** vector field that points pixels towards the joint
** pixels within the offset maps vote on where the joint really is
*** weighted average of the candidate locations


.Weighted average of estimated positions
[stem]
++++
F_k (x_i) = \sum_j \frac{G(x_j + F_k(x_j) - x_i) h_k(x_j)}{\pi R^2} 
++++

where:

* stem:[x_i] is the position in which to compute the output value
* stem:[x_j] are other positions
** typically near stem:[x_i]
* stem:[x_j + F_k(x_j)] is the estimated position of the stem:[k_th joint]
** stem:[x_j + ] offset
* stem:[x_j + F_k(x_j) - x_i] is the distance between the estimate and stem:[x_i]
* stem:[G] is a linear interpolation kernel
** 1 pixel wide tent function around stem:[x_i]
** stem:[G(0, 0) = 1]
* stem:[G(x_j + F_k(x_j) - x_i)] is the the contribution to the vote
** how confident it is that stem:[x_i] is the joint location
* stem:[h_k(x_j)] is the condidence of the predictions
** weighs the vote
** high confidence = heavier

== Single person 3D HPE

Given an RGB image you can get 2 things:

* skeleton based -> 3D keypoints
* model based -> 3D mesh/model
** can infer keypoints from surface

Use volumetric heatmaps instead of 2D heatmaps

Uses *Geodesic Point Similarity* instead of OKS

* similar to OKS
* works on 3D data

.OKS formulation
[stem]
++++
\text{OKS} =
\frac
{
    \sum_i \exp \biggl(-\frac
    {d^2_i}{2s^2 \cdot k_i^2}\biggr)
    \mathbb{I} [v_i \gt 0]
}    
{
    \sum_i \mathbb{I} [v_i \gt 0]
}
++++

where:

* stem:[d_i] is the Euclidian distance between the predicted point and the ground truth point
* stem:[s] is the scale of the object
** i.e. stem:[\sqrt{\text{area of segmentation}}]
* stem:[k_i] is a constant that controls falloff
* stem:[v_i] is the ground truth visibility flag
** COCO:
*** stem:[v_i = 0 \implies] not visible
*** stem:[v_i = 1 \implies] labeled but not visible
*** stem:[v_i = 2 \implies] visible

i.e.:

* while ignoring all invisible and unlabeled keypoints (stem:[\mathbb{I} [ v_i \gt 0 \]])
* pass the distance stem:[d_i] through a Gaussian to give a measure of similarity
** Gaussian standard dev is stem:[\sigma = sk_i]
** similarity is value in stem:[[0, 1\]], where stem:[d_i \sim 0] is very good similarity

