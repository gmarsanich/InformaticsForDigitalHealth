= Virtual Worlds - Fundamentals of Computer Graphics 
:toc:
:toc-title: Contents
:nofooter:
:stem: latexmath

== 3D geometry representations

=== Why does geometry matter in healthcare?

Need accurate geometric representation of real objects

* needs to be extremely close to the real thing

=== Graphics pipeline

Data passes through a chain of computational nodes that transforms it with every step until we get an output

Transform data into computer image

5 overall stages (actually many more irl)

. Application stage
. Geometry Processing
* assemble the data from application stage using mathematical/geometric concepts 
. Rasterization
* convert geometric primitives to draw the thing on the screen
* converts continuous to discrete
** i.e. makes a grid
* very fast GPU algorithm
* not physically accurate
. Fragment Shading
* identify pixel information
* identify how to process the fragment
** texture, color, vfx
. Output Merging
* put it all together

image::image.png[title=Basic graphics pipeline]

Multiple buffers used to generate final images

.Depth buffer for 3D effect
* certain pixels need to go behind/in front of others
* stores the pixels nearest to the observer
* initialize with 1 and test if pixel is within a certain range
** if > range then draw it
** if < range then dont't draw it (it's behind something else)
* basically sorts primitives

sometimes pixels are interleaved and we don't know which goes in front
simple but expensive algorithm

== Mathematical Projection Systems

Use 100 year old concepts on modern graphics

=== Orthographic projection

Projecting 3D objects in 2D

No depth, everything is parallel to the observer

* the sun is so far from earth that we can consider the rays to be parallel when they hit the earth
** they aren't actually parallel because the sun is a sphere but it's so far that it doesnt really matter

This results in no perspective distortion

Also preserves measurements

X-rays use orthographic mapping

* we don't want to distort perspective

=== Perspective projection

Uses converging projection lines

* warps the object to simulate depth
* like that thing you do in art class with focus points/lines

Object size scales with distance

* farther => smaller
* closer => bigger

Similar to human vision

3D visualization typically use perspective projection

* brain scans

== Rendering Approaches

=== Rasterization 

Projects geometric primitives onto screen pixels

Fast but inaccurate

* good for real time use

=== Ray Tracing

Simulates photons

Shoot simulated photon

* If it hits an object then it's lit

In practice we use reverse ray tracing

* cast ray from observer and see if it reaches light source
** if reaches light source then it's lit
** else unlit

Uses the first normal to determine whether the object is hit or not

Enables shadows, reflections and refractions for free basically

Much more expensive than rasterization

* not really used for real time visualization (despite NVIDIA's best efforts)
* more commonly used for still imagery

=== Modern rendering techniques

.Deferred rendering
Separates geometry and lighting passes

Do expensive operations only when necessary

.Global illumination
Simulates realistic light bouncing and ambient occlusion

* when a ray hits an object that object becomes a light itself

.Real time RT and Path Tracing
Very computationally expensive

Very complex to use

Most realistic

== Vertex fundamentals 

How do we specify something real in (a) space?

* store its position
** `(x, y, z)` coordinates
** changes with coordinate system
*** there isn't a standard coordinate system
*** latiude/longitude/altitude are conventional -> not cartesian (banded artesian)
*** coordinate systems are created almost arbitrarily
*** terra centric, helio centric, flat projections (Mercator)

* represent color
** in some way, usually RGB(A)
*** HSL exists

* normal vector
** surface orientation for lighting calculations
** for a mirror: angle of incidence = angle of eccedence
*** comes in at 45 degrees and leaves at 45 degrees

* texture coordinates
** `(u, v)` coordinates
** take 3D mesh and splat it on plane

=== Vertex attributes in WebGL

Web version of OpenGL

[source,javascript]
----
// WebGL vertex

gl.vertexAttribPointer(
    positionLocation,
    3,
    gl.FLOAT,
    false,
    0,
    0
)
----

== Geometric primitives

=== Points

A single point is represented by a single vertex

* sparse data representation
* individual scan points
** point cloud
* particle systems

Most primitive of the primitives

=== Lines

2 points joined together (not really)

* technically segments
* wireframes
* vector fields
* anatomical pathways

Needs 2 vertices

=== Triangles

First type of *surface*

need 3 points

the convexly enclosed space withn those points makes a surface

GPUs are good at triangles

Triangles can be used to generate all kinds of polygons

* guaranteed to be planar
* can be assembled into any shape
** a square is just 2 triangles arranged in some way
** a hexagon is the same but 6

== Coordinate System Hierarchy

Using matrix multiplications we can move objects around in a space

There are different spaces

=== Local/Object space

Local model definition relative to the object's origin

The coordinate space local to an object

* basically the origin coordinates of an object within its space

For example if you made a cube in Blender at `(0, 0, 0)` its local coordinates would be `(0, 0, 0)` despite of where it's placed in the final application

=== World space

The coordinate space local to the world

* if we imported a bunch of objects into the world directly they'd probably all end up at `(0, 0, 0)` (cringe)
* the local coordinates of each object have to be transformed into the world space's coordinates

Local coordinates are transformed into world coordinates using the *model matrix*

=== View/Camera/Eye space

The transformation of the world space coordinates into objects visible to the observer

* objects are manipulated in some way such that they appear to be in front of the camera
** translation
** rotation

World coordinates are transformed into view coordinates using the *view matrix*

=== Clip space

The space where objects with coordinates outside a specified range are put

* say our world space has a range of `(-1000, 1000)` coordinates in each axis
* an object with coordinates `(-1001, 1001, 1001)` would have its excessive edges clipped off
** e.g. like the tip of a triangle or whatever
*** OpenGL normalizes this to be between `(-1, 1)`

This creates a viewing box called a *frustum*

* anything within the box is drawn
* the rest is clipped

==== NDC

The 3D clip space coordinates are divided by the homogenous vector's 4th item (`w`) to produce normalized device coordinates

* a cube stem:[[ -1, +1 \]^3]

=== Screen space

Transforms the NDC coordinates into the same pixel grid that the rasterizes draws into

* where the GPU draws stuff

== Transformation matrix chain

Start in object space

Using the model matrix transform the object space into world space

* gives world coordinates

stem:[V_{world} = M_{model} \cdot V_{object}]

Camera transform transforms world coords to camera coords

stem:[V_{camera} = M_{view} \cdot V_{world}]

Camera coords are then converted into clip space

stem:[V_{clip} = M_{proj} \cdot V_{camera}]

The output is converted into normalized coordinates so it can be drawn on screen

== Homogenous coordinates

Represent N-D coordinate n-ples as n+1 ples

* 2D `(x, y)` -> `(x, y, w)`
* 3D `(x, y, z)` -> `(x, y, z, w)`

In principle `w` can be any nonzero value

* if stem:[w = 0] -> its vector is a direction
** an unreachable point
* if stem:[w \neq 0] -> its vector is a point
** any nonzero value means it's a point
** only the scale changes

`w` allows nonlinear computations to become vector or matrix operations

* easy computations
* very optimized

Object matrix + matrix

When you go from clip space to screen space you take the clip matrix and use `w` to represent everything between the eye of the observer and a given line

* no idea what this means btw

Divide `xyz` by `w` to get the screen coordinates

=== Point Clouds

=== Point Cloud Fundamentals

We have to assemble primitives into something we can look at

* points
* lines
* triangles

A point cloud is a collection of points

* unordered collection of 3D points
* a point is a reference to a vertex
* thus a point cloud is a list of vertices

Point clouds are generated by:

* LIDAR
* MRI/CT scans
* Photogrammetry

=== Point Cloud pros and cons

.Pros
High resolution acquisition

* they collect a lot of data
* more data than possible for other methods

Can go straight into ML training

* no geometric preprocessing required

.Cons
* Samples of a surface
** points exist independently
* No normal information
** need additional work to get that

== Surface representations

Using meshes 

* collection of connected triangles
* each triangle shares an edge with another triangle

Using enough triangles we can obtain a very good approximation of a surface

=== Mesh Classification

NOTE: These all use triangles at some point

.Triangular meshes
As before

.Quad models
Made with 4 sided objects

* rectangles
* squares

Easier to edit

.Polygonal meshes
use more abstract shapes for more specialized applications

=== Mesh Data Organization

. List of vertices
* coordinates for each point in mesh
. List of edges
* connections between vertices
* define mesh topology
. List of faces
* 
. Additional data

=== Mesh Processing Operations

Apply algorithms to triangles

.Simplification (LOD)

Use fewer triangles to represent objects that are too far from the observer for them to notice

* this saves resources

.Smoothing

Remove noise from data

Fix aliasing

.Subdivision surfaces

Subdivides the faces of a surface into smaller faces

Enables higher definition

== Volumetric Representations

Fixed grid where you grab everything on every voxel of a volume

* voxel = 3D pixel

Basically the inside of a mesh

Much more expensive than a mesh

* grows cubically instead of quadratically

* 512^3 resolution eats 4GB of memory

=== Voxel grid math

Voxel size determines resolution and affects memory usage

* higher size 
** lower resolution
** lower memory usage

* smaller size 
** higher resolution
** higher memory usage

// Store position in space

// Store what sensor is able to read

// ?????

=== Volume rendering techniques

.Direct Volume Rendering
Cast ray through volume where opacity < 1 and record everything

.Isosurface extraction
use the marching cubes algorithm

* creates mesh surfaces from volumes
* density can be set

.Hybrid rendering

Mixes both

=== Skeletal Structures

hierarchical geometric structure

* uses bone chains and joint relationships
* considers torso as the source
** can change depending on application

Tree/graph structure

==== Healthcare Applications

* Joint mechanics
* Prosthetics fitting
* Physical therapy

=== Skinning and Deformation

Rigs how a vertex moves according to a bone movement

* warp the surface when the angle between 2 bones changes

== Material Properties

3 main components

.Albedo
Base surface color

.Reflectivity
Amount of reflected/absorbed light

.Roughness
Micro structures that affect light scattering and glossiness

=== Texture mapping fundamentals

Any texture can be mapped on any surface

Projects 2D mimages onto 3D images using `(u, v)` coordinates

* where stem:[(u, v) \in [ 0, 1\]^2]

* using *normal mapping*

== Lighting 

=== Phong model

Cheaper

Ambient light simulates ambient occlusion

Light can be diffused depending on the angle between the angle of the ray and the normal of the surface

Specular light is light that moves back into the observer's view from an object

=== Physically Based Rendering (PBR)

more realistic

based on physical properties

== Shader programming

use GPU and specialized programs to write lighting

.Vertex shader
processes individual vertices

* coordinate transformations
* lighting
* attributes

.Fragment shader
handles material properties

* colors
* textures
* etc.

=== Shader Techniques

.1. Normal mapping
* simulates surface details
* no extra geometry required

.2. Parallax mapping
* creates illusion of depth
* more realistic

.3. Subsurface scattering
* allows light to penetrate surfaces
* penetration depth varies depending on parameters
** strength of ray
** material type
** material reflectance
** material thickness

=== Medical Shader Applications

.1. Tissue color coding
* assigning color based on conditions
** tissue type
** pathology
** other parameters

.2. Value Thresholding
* filtering/highlighting specific measurement values (?)

.3. Interactive segmentation
* users can select/isolate specific structures

== Healthcare visualization

Healthcare data is sensitive so it's important to represent it properly

=== Hybrid representation strategies

Use the most appropriate representation technique for the target

. meshes for bones
* solid* structures
** if we ignore the marrow I guess
* clear details

. volumes for organs
* internal detail required

== Healthcare visualization pipeline

. Data acquisition
* imaging data is required
** CT
** MRI
** ultrasound
** x-ray

. Tissue segmentation
* proper visualization requires knowing what is being visualized
* organs need to be separated from bones/muscles/whatever
** can do manually (slow)
** can use AI (fast)

. Geometry conversion
* segmented data is meaningless
* needs to be converted into geometric representation
** volumes
** meshes
** etc.

GPUs are designed for this so they should be used if available

* they should be acquired if unavailable (good luck with that these days)
