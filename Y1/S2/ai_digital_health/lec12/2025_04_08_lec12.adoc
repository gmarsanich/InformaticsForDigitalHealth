= AIDH - Deep Learning for Medical Imaging
:toc:
:toc-title: Contents
:nofooter:
:stem: latexmath

== Introduction to Medical Imaging

=== General Image Data

Matrix/tensor of pixels

* e.g. grayscale image has gray intensity value stem:[\in \{0, 255\}]

Different channels have different values

=== Biomedical Images & Vision Tasks

Not just simple photos/images

Encompass different modalities and spectra

* spectral
* temporal
* spatiotemporal

Noisy

* ultrasounds are operator dependent despite guidelines

Imaging modalities include:

* radiology
** xray
** CT
** MRI
* oncology
** PET scans
** specialized MRI
* pathology
** slides/slices of tissue
* ultrasounds

Common tasks include:

* classification
** given image does X have cancer or not
* regression
** predict time to event, tumor size after n months, etc.
* segmentation and detection
** finding parts of interest
** finding and naming tumors (detection)
** delineating tumors (segmentation)
* registration
** aligning image structures from different images
* enhancement
** upscaling
** denoising
** artifact removal
*** removing the skull in MRI

=== Physics of Biomedical Imaging

Distortion of some wave produces image 

Reconstructing an image from a signal is called an *inverse problem*

* convert this into interpretable data
* if photons then can use ray tracing

inverting a *sinogram* to get image

sinogram is generated by convolution

== Convolutional Neural Networks

=== Why?

Processing images with MLP is horrendously inefficient

32x32 icon has 3000 parameters 

Images are very high dimensional data

Have to do it efficiently

MLPs are positional - you want to be able to recognize a part of image irrespective of where it is

*need a way to make parameters independent of pixels* while keeping pixel context

* 25 pixels next to each other have a meaning

=== Compositionality

Neural Networks work by composition

as we go deeper information becomes more abstract with higher level semantics

=== Convolution

Convolution is a linear combination operator

Element x element multiplication followed by summation

.Convolution
[stem]
++++
g_{x, y} = \omega \cdot f_{x,y} = \sum_{i = -a}^a \sum_{j = -b}^b \omega_{i, j} \cdot f_{x-i, y-j}
++++

Where:

* stem:[g(x, y)] is the filtered image
* stem:[f(x, y)] is the original image
* stem:[\omega] is the filter kernel
* stem:[a] and stem:[b] are kernel elements

Followed by a filter (kernel with weights stem:[w_i])

* matrix of parameters
* 1 filter is 1 neuron
* odd size and overlapping

Apply filter to the first valid pixel starting from somewhere (top left is good)

Apply it such that it doesn't go outside the border

Filter returns value stem:[c] that is saved to the first pixel of the new image

Slide filter 1 unit (in this case 1 pixel to the right)

* can move stem:[n] elements at a time depending on the *stride hyperparameter*
* reduces the amount of multiplications at the cost of less precision
* makes it easier to correlate information from more distant pixels
* 0 pad the image to keep original image size
** based on filter size stem:[P = \frac{k-1}{2}] where stem:[k] is the size of the filter
** usually used with stride = 1 unless otherwise required

Repeat until all pixels are filtered

The filter is reused for all pixels

Much more efficient (in this case 9 parameters instead of 3000)

Output will be another image (usually smaller)

Also known as feature map

* i.e. input image as perceived by the filter

For multi channel images (e.g. color images) the filter needs to be applied to all channels and the output will still be a 1 channel feature map

* *TYPICALLY*, if channels are things you want to mix
* for medical imaging you might want to keep them separated until a later point

Convolution requires a nonlinear activation function to transform the feature map

* done pixel by pixel using activation function stem:[\phi]
* standard stem:[\phi] is ReLU

=== Pooling

Pooling has no parameters*, but pooling size is a hyperparameter

* *pooling size is very standardized
* *usually tuned manually only if required

Spatial pooling for 1 channel images

CNNs are not homogenous

Pooling aggregates values inside a filter to gain spatial invariance

* use stride = 2 to avoid overlapping pools for pixel neighborhoods

Pooling filters are usually even sized and non overlapping

Max pooling is most common but other functions exist

* mean pooling also exists

For stem:[n] channel images perform pooling with same region across all stem:[n] channels at once

=== Combining the Ingredients

Many ways to combine ingredients

Input -> convolution -> relu -> pooling

Often there are multiple convolutions and relu before pooling

=== Combining Convolutional Filters

Having one filter is useless

For 2 filters:

* apply filters separately on all channels
* Returns 2 convolutions
* Concatenate outputs
* End up with 2 channel feature map (1 channel per convolution)

=== Prototypical Convolutional Architecture

Combine CL layers into smaller and smaller images with more and more filters to represent increasingly abstract features

Throw output of last CL into fully connected layer to fuse information stored in each layer of the last CL

Neurons are densely connected

There are many points in CL[-1] (MANY points) which means many parameters

=== Useful Architecture Tricks

==== Data Augmentation

Generates new images using real existing images

Given image stem:[I] with label stem:[L]:

* stem:[I] can be:
** flipped
** zoomed
** cropped
** recolored
** etc
* the label will be the same

Be *very* careful with biomedical images

* Can't use the same augmentation techniques on MRI that you use on CT
* Can't use some techniques at all
** cutouts on tumor images are bad
** if we cut out the tumor then we have nothing

Think carefully about what techniques you can use

==== Transfer Learning

Don't have to train CNN from scratch

Find CNN trained on large amounts of data

* e.g. model trained to classify cars and faces

Find parts not connected to predictions

* i.e. convolution parts

Replace the prediction parts with bits that will be trained on your new images

.Rules of Thumb

If need to fine tune on small dataset you can just change the final output layer

If the dataset is bigger you can replace a bit more layers

Careful it doesn't memorize training data when fine tuning

* use small learning rates

Integrating other data with images is kind of involved:

* let the CNN get to the fully connected layer
* inject new information at the fully connected layer
* this would be a *late fusion multimodal* model
** multimodal: image+genomic data (for example)
** late fusion: fusing different modalities late in the model 

== Medical Imaging Tasks

Different tasks require different models

=== Classification

Finding which type of cancer is being observed

Is this person affected by X or not?

=== Regression

Predicting tumor volume, disease spread

=== Segmentation and detection

Find area of image with interesting object

Detection draws a box around the area

Segmentation tries to separate the area with a pixel size border

2 types of segmentation:

* Semantic
* forgor

==== Segmentation Architectures

U-net is a CNN for biomedical image segmentation

* Doesn't need that much input data
* Easily adapts to 3D images

Produces image of same size as input image

For each pixel we have a value between 0 and 1

* 0: normal tissue
* 1: abnormal tissue

Generates a prediction for each pixel

. Few convolutional layers with different resolutions
. 2x2 max pooling to shrink convolution output
. Repeat until bottom
* Most compressed representation of image (single vector)
* Very rich in information
* Have to go back to input size (how?)
* Do the inverse of pooling and convolving
** upsampling and deconvolving (or up convolving)
** project small image onto a big image (using 0 padding)
** 0 pad strided pixels as well if used stride before
. Up convolv/upsample vector back into input size matrix
. Use skip/copy to propagate input image structure into the upconv steps (residual connection)
. Train with classification loss
* *SCALE THE LOSS* because normal tissue will almost always be more common than abnormal tissue

==== Assessing Image Segmentation

Use intersection over union (aka Jaccard index)

Ratio between intersection (pixel in both target and prediction) and union of target and prediction

.Jaccard Index
[stem]
++++
IoU = \frac{\text{target} \cap \text{prediction}}{\text{target} \cup \text{prediction}}
++++

==== Dilated Convolution

Center filter on some pixel

Dilate pixels by some factor

Filter is applied by skipping filters and convolve the whole thing

Outputs map of same size as input image

Lets the filter see a larger area which gives it more information

==== Detection: faster R-CNN

Takes image

Passes it to CNN

Returns feature map of interesting pixels

Passes map to *region proposal network* that tries to draw boxes around object

Can be used for video surgery but it *must* be fast

* YOLO (You Only Look Once)
** gives area of interest in the same stage as region proposal generation
** Fast but lower performance
* RetinaNet
** uses focal loss
** faster but similar performance?